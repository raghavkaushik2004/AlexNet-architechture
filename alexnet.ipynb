{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUfHKJhH4XEk4PmLghF1uu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raghavkaushik2004/AlexNet-architechture/blob/main/alexnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "bKFQIKeX2FkQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras import datasets, layers, models, losses\n",
        "from tensorflow import newaxis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the dataset MNIST has the dimensions of 28x28, so for making the number of channels = 3, we repeat it 3 times to get 28x28x3. So we have to reshape it to 227x227x3. The paper says it is 224x224x3, but for the math to come out right(The original paper said different numbers, but Andrej Karpathy, the former head of computer vision at Tesla, said it should be 227×227×3 (he said Alex didn't describe why he put 224×224×3)). So, we reshape the image size to 227x227x3."
      ],
      "metadata": {
        "id": "7M-iF3Co7Ekc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train = tf.pad(x_train, [[0, 0], [98, 97], [98, 97]]) / 255\n",
        "# x_train = tf.expand_dims(x_train, axis = -1)\n",
        "# x_train = tf.repeat(x_train, 3, axis = -1)\n",
        "# x_train = tf.image.resize(x_train, (255, 255))\n",
        "\n",
        "# Colab crashes due to RAM constraints"
      ],
      "metadata": {
        "id": "jBymKu6u63mw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "x_train = tf.image.resize(x_train, (227, 227))\n",
        "x_test = tf.image.resize(x_test, (227, 227))\n",
        "x_train = x_train[... , tf.newaxis]\n",
        "x_test = x_test[... , tf.newaxis]\n",
        "x_train = tf.repeat(x_train, 3, axis = -1)\n",
        "x_test = tf.repeat(x_test, 3, axis = -1)"
      ],
      "metadata": {
        "id": "bTcQWzy0GgvO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()"
      ],
      "metadata": {
        "id": "uoHJ0TprHDi-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Bilinear Interpolation to resize the image"
      ],
      "metadata": {
        "id": "dFUrFWyjmhcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(layers.experimental.preprocessing.Resizing(227, 227, interpolation=\"bilinear\", input_shape=x_train.shape[1:]))\n",
        "\n",
        "# 227x227x3 is now converted to 55x55x96 using\n",
        "model.add(layers.Conv2D(96, 11, strides = 4, padding = \"same\"))\n",
        "model.add(layers.Lambda(tf.nn.local_response_normalization))\n",
        "model.add(layers.Activation('relu'))\n",
        "\n",
        "# max pooling layer applied on 55x55x96 with 3x3 window and stride = 2 to obtain an img of 27x27x96\n",
        "model.add(layers.MaxPooling2D(3, strides = 2))\n",
        "\n",
        "# 'same' convolution with 256 5x5 filers to get img of size 27x27x256\n",
        "model.add(layers.Conv2D(256, 5, strides = 4, padding = \"same\"))\n",
        "model.add(layers.Lambda(tf.nn.local_response_normalization))\n",
        "model.add(layers.Activation('relu'))\n",
        "\n",
        "# max pooling layer applied on 27x27x256 with 3x3 filter and stride = 2 to obtain an img of 13x13x256\n",
        "model.add(layers.MaxPooling2D(3, strides = 2))\n",
        "\n",
        "# 2 layers of 'same' convolution with 384 3x3 filters which makes the dim of the img to 13x13x384\n",
        "model.add(layers.Conv2D(384, 3, padding = \"same\"))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.Conv2D(384, 3, padding = \"same\"))\n",
        "model.add(layers.Activation('relu'))\n",
        "\n",
        "# one 'same' convolution with 256 3x3 filter which makes the dim of the img to 13x13x256\n",
        "model.add(layers.Conv2D(256, 3, padding = \"same\"))\n",
        "model.add(layers.Activation('relu'))\n",
        "\n",
        "# max pooling layers with 6x6 window and stride = 2 to obtain an img of size 6x6x256\n",
        "model.add(layers.MaxPooling2D(3, strides = 2))\n",
        "\n",
        "# flattening the data into one-dimensional vector\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "# first fully connected layer"
      ],
      "metadata": {
        "id": "Yw1j-sDDmZbj"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FLem9YocghJ_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x-d9soEKwD7G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}